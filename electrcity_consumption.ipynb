{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gEHV5JTthrdI",
        "outputId": "1725d3ec-1313-4a59-a153-580d1e673534"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset\n",
        "train_df = pd.read_csv('train.csv')\n",
        "test_df = pd.read_csv('test.csv')\n",
        "\n",
        "# Display basic Information about the training data\n",
        "print('Train Data info:')\n",
        "train_df.info()\n",
        "print('\\nTrain Data head:')\n",
        "train_df.head()\n",
        "\n",
        "print('\\nTrain Data Description')\n",
        "print(train_df.describe())\n",
        "\n",
        "# Display basic Information about the test data\n",
        "print('Test Data info:')\n",
        "test_df.info\n",
        "print('\\nTest Data head:')\n",
        "test_df.head()\n",
        "\n",
        "print('\\nTest Data Description')\n",
        "print(test_df.describe())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7VL_aD2bilaU"
      },
      "outputs": [],
      "source": [
        "# Convert 'date' column to datetime objects\n",
        "train_df[\"date\"] = pd.to_datetime(train_df[\"date\"])\n",
        "test_df[\"date\"] = pd.to_datetime(test_df[\"date\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nK97Boq8i5pe",
        "outputId": "2b35a988-32d7-4075-f39c-3a1fb53a5fde"
      },
      "outputs": [],
      "source": [
        "# Extract time-based features\n",
        "train_df[\"year\"] = train_df[\"date\"].dt.year\n",
        "train_df[\"month\"] = train_df[\"date\"].dt.month\n",
        "train_df[\"day\"] = train_df[\"date\"].dt.day\n",
        "train_df[\"day_of_week\"] = train_df[\"date\"].dt.dayofweek\n",
        "train_df[\"day_of_year\"] = train_df[\"date\"].dt.dayofyear\n",
        "train_df[\"week_of_year\"] = train_df[\"date\"].dt.isocalendar().week.astype(int)\n",
        "train_df[\"quarter\"] = train_df[\"date\"].dt.quarter\n",
        "train_df[\"is_weekend\"] = train_df[\"date\"].dt.dayofweek.isin([5, 6]).astype(int)\n",
        "\n",
        "test_df[\"year\"] = test_df[\"date\"].dt.year\n",
        "test_df[\"month\"] = test_df[\"date\"].dt.month\n",
        "test_df[\"day\"] = test_df[\"date\"].dt.day\n",
        "test_df[\"day_of_week\"] = test_df[\"date\"].dt.dayofweek\n",
        "test_df[\"day_of_year\"] = test_df[\"date\"].dt.dayofyear\n",
        "test_df[\"week_of_year\"] = test_df[\"date\"].dt.isocalendar().week.astype(int)\n",
        "test_df[\"quarter\"] = test_df[\"date\"].dt.quarter\n",
        "test_df[\"is_weekend\"] = test_df[\"date\"].dt.dayofweek.isin([5, 6]).astype(int)\n",
        "\n",
        "# Create temperature range feature\n",
        "train_df[\"temperature_range\"] = train_df[\"temperature_2m_max\"] - train_df[\"temperature_2m_min\"]\n",
        "test_df[\"temperature_range\"] = test_df[\"temperature_2m_max\"] - test_df[\"temperature_2m_min\"]\n",
        "\n",
        "# One-hot encode 'cluster_id'\n",
        "train_df = pd.get_dummies(train_df, columns=[\"cluster_id\"], prefix=\"cluster\")\n",
        "test_df = pd.get_dummies(test_df, columns=[\"cluster_id\"], prefix=\"cluster\")\n",
        "\n",
        "# Save processed data\n",
        "train_df.to_csv(\"train_processed.csv\", index=False)\n",
        "test_df.to_csv(\"test_processed.csv\", index=False)\n",
        "\n",
        "print(\"Feature engineering complete. Processed files saved.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hsg4B0L7jDC2"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Ea208-tjN8N",
        "outputId": "0b0d235b-e75c-472b-f59c-4931e1a11f86"
      },
      "outputs": [],
      "source": [
        "train_cols = [col for col in train_df.columns if col.startswith(\"cluster_\")]\n",
        "test_cols = [col for col in test_df.columns if col.startswith(\"cluster_\")]\n",
        "\n",
        "\n",
        "missing_in_test = set(train_cols) - set(test_cols)\n",
        "for c in missing_in_test:\n",
        "    test_df[c] = 0\n",
        "\n",
        "missing_in_train = set(test_cols) - set(train_cols)\n",
        "for c in missing_in_train:\n",
        "    train_df[c] = 0\n",
        "\n",
        "# Define features (X) and target (y)\n",
        "features = [col for col in train_df.columns if col not in [\"ID\", \"date\", \"electricity_consumption\"] and not col.startswith(\"cluster_id\")]\n",
        "features.extend([col for col in train_df.columns if col.startswith(\"cluster_\")])\n",
        "\n",
        "X = train_df[features]\n",
        "y = train_df[\"electricity_consumption\"]\n",
        "\n",
        "# Split data for training and validation\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize and train the model\n",
        "model = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the validation set\n",
        "val_preds = model.predict(X_val)\n",
        "\n",
        "# Evaluate the model using RMSE\n",
        "rmse = np.sqrt(mean_squared_error(y_val, val_preds))\n",
        "print(f\"Validation RMSE: {rmse}\")\n",
        "\n",
        "# Prepare test data for prediction\n",
        "X_test = test_df[features]\n",
        "\n",
        "# Make predictions on the test set\n",
        "test_preds = model.predict(X_test)\n",
        "\n",
        "\n",
        "submission_df = pd.DataFrame({\n",
        "    \"ID\": test_df[\"ID\"],\n",
        "    \"electricity_consumption\": test_preds\n",
        "})\n",
        "\n",
        "\n",
        "submission_df[\"electricity_consumption\"] = submission_df[\"electricity_consumption\"].apply(lambda x: max(0, x))\n",
        "\n",
        "submission_df.to_csv(\"submission.csv\", index=False)\n",
        "\n",
        "print(\"Model training and submission file generation complete.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qy9aeK6JjYYo"
      },
      "outputs": [],
      "source": [
        "import lightgbm as lgb\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VtFynIYmjkWb",
        "outputId": "80414340-98fa-4fab-e701-7814a0931ca9"
      },
      "outputs": [],
      "source": [
        "all_cluster_cols = sorted(list(set([col for col in train_df.columns if col.startswith(\"cluster_\")]).union(\n",
        "                                 set([col for col in test_df.columns if col.startswith(\"cluster_\")]))))\n",
        "\n",
        "# Add missing cluster columns to train_df and test_df with 0\n",
        "for col in all_cluster_cols:\n",
        "    if col not in train_df.columns:\n",
        "        train_df[col] = 0\n",
        "    if col not in test_df.columns:\n",
        "        test_df[col] = 0\n",
        "\n",
        "\n",
        "base_features = [col for col in train_df.columns if col not in [\"ID\", \"date\", \"electricity_consumption\"] and not col.startswith(\"cluster_id\")]\n",
        "\n",
        "\n",
        "features = sorted(list(set(base_features).union(set(all_cluster_cols))))\n",
        "\n",
        "\n",
        "common_features = list(set(features) & set(train_df.columns) & set(test_df.columns))\n",
        "\n",
        "X = train_df[common_features]\n",
        "y = train_df[\"electricity_consumption\"]\n",
        "\n",
        "X_test = test_df[common_features]\n",
        "\n",
        "# LightGBM Model Training with Cross-Validation\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "oof_preds = np.zeros(len(X))\n",
        "test_preds = np.zeros(len(X_test))\n",
        "\n",
        "for fold, (train_index, val_index) in enumerate(kf.split(X, y)):\n",
        "    print(f\"Fold {fold+1}\")\n",
        "    X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n",
        "    y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
        "\n",
        "    lgb_params = {\n",
        "        'objective': 'regression_l1',\n",
        "        'metric': 'rmse',\n",
        "        'n_estimators': 1000,\n",
        "        'learning_rate': 0.05,\n",
        "        'feature_fraction': 0.8,\n",
        "        'bagging_fraction': 0.8,\n",
        "        'bagging_freq': 1,\n",
        "        'lambda_l1': 0.1,\n",
        "        'lambda_l2': 0.1,\n",
        "        'num_leaves': 31,\n",
        "        'verbose': -1,\n",
        "        'n_jobs': -1,\n",
        "        'seed': 42 + fold\n",
        "    }\n",
        "\n",
        "    model = lgb.LGBMRegressor(**lgb_params)\n",
        "    model.fit(X_train, y_train,\n",
        "              eval_set=[(X_val, y_val)],\n",
        "              eval_metric='rmse',\n",
        "              callbacks=[lgb.early_stopping(100, verbose=False)])\n",
        "\n",
        "    oof_preds[val_index] = model.predict(X_val)\n",
        "    test_preds += model.predict(X_test) / kf.n_splits\n",
        "\n",
        "# Evaluate OOF predictions\n",
        "rmse_oof = np.sqrt(mean_squared_error(y, oof_preds))\n",
        "print(f\"Overall OOF RMSE: {rmse_oof}\")\n",
        "\n",
        "\n",
        "submission_df = pd.DataFrame({\n",
        "    \"ID\": test_df[\"ID\"],\n",
        "    \"electricity_consumption\": test_preds\n",
        "})\n",
        "\n",
        "# Ensure predictions are non-negative\n",
        "submission_df[\"electricity_consumption\"] = submission_df[\"electricity_consumption\"].apply(lambda x: max(0, x))\n",
        "\n",
        "submission_df.to_csv(\"submission.csv\", index=False)\n",
        "\n",
        "print(\"Model optimization and submission file generation complete.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c8666a83"
      },
      "source": [
        "# Task\n",
        "Optimize the RMSE of the LightGBM model to be between 10 and 20, ensuring the model does not overfit, and the predictions are accurate for actual values, specifically for examples like \"cluster_1_2022-01-01,0\", \"cluster_2_2022-01-01,0\", and \"cluster_3_2022-01-01,0\". Use the formula $\\text{RMSE} = \\sqrt{ \\frac{1}{n} \\sum_{i=1}^{n} \\left( \\hat{y}_i - y_i \\right)^2 }$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c01783ea"
      },
      "source": [
        "## Feature engineering refinement\n",
        "\n",
        "### Subtask:\n",
        "Explore creating more advanced features or transforming existing ones to capture complex relationships in the data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b71c3605"
      },
      "source": [
        "**Reasoning**:\n",
        "I will create new features based on the date and existing numerical columns to capture more complex relationships in the data. This includes creating interaction terms, polynomial features, and lag features.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9cbc564a"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous command failed because 'cluster_id' was one-hot encoded and removed from the dataframe. I need to sort by the individual cluster columns instead of the original 'cluster_id' column to create lag features.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 701
        },
        "id": "6dy-vglQ6cUJ",
        "outputId": "d9925c44-eb2b-46ae-ae9d-ed5a9b117527"
      },
      "outputs": [],
      "source": [
        "# Create interaction features\n",
        "train_df[\"temp_daylight_interaction\"] = train_df[\"temperature_2m_max\"] * train_df[\"daylight_duration\"]\n",
        "test_df[\"temp_daylight_interaction\"] = test_df[\"temperature_2m_max\"] * test_df[\"daylight_duration\"]\n",
        "\n",
        "train_df[\"temp_sunshine_interaction\"] = train_df[\"temperature_2m_max\"] * train_df[\"sunshine_duration\"]\n",
        "test_df[\"temp_sunshine_interaction\"] = test_df[\"temperature_2m_max\"] * test_df[\"sunshine_duration\"]\n",
        "\n",
        "# Create polynomial features for temperature\n",
        "train_df[\"temperature_2m_max_sq\"] = train_df[\"temperature_2m_max\"]**2\n",
        "test_df[\"temperature_2m_max_sq\"] = test_df[\"temperature_2m_max\"]**2\n",
        "\n",
        "train_df[\"temperature_2m_min_sq\"] = train_df[\"temperature_2m_min\"]**2\n",
        "test_df[\"temperature_2m_min_sq\"] = test_df[\"temperature_2m_min\"]**2\n",
        "\n",
        "# Sort by date and the cluster columns for creating lag features\n",
        "cluster_cols = [col for col in train_df.columns if col.startswith(\"cluster_\")]\n",
        "train_df = train_df.sort_values(by= cluster_cols + [\"date\"])\n",
        "test_df = test_df.sort_values(by= cluster_cols + [\"date\"])\n",
        "\n",
        "\n",
        "# Create lag features for electricity consumption (training data only) and temperature features\n",
        "train_df[\" बिजली_consumption_lag_1\"] = train_df.groupby(cluster_cols)[\"electricity_consumption\"].shift(1)\n",
        "test_df[\"temperature_2m_max_lag_1\"] = test_df.groupby(cluster_cols)[\"temperature_2m_max\"].shift(1)\n",
        "test_df[\"temperature_2m_min_lag_1\"] = test_df.groupby(cluster_cols)[\"temperature_2m_min\"].shift(1)\n",
        "\n",
        "\n",
        "# Fill NaNs created by shifting\n",
        "train_df.fillna(method='bfill', inplace=True)\n",
        "test_df.fillna(method='bfill', inplace=True)\n",
        "\n",
        "train_df.fillna(method='ffill', inplace=True)\n",
        "test_df.fillna(method='ffill', inplace=True)\n",
        "\n",
        "# Display head of modified dataframes\n",
        "print('Train Data head after feature engineering:')\n",
        "display(train_df.head())\n",
        "\n",
        "print('\\nTest Data head after feature engineering:')\n",
        "display(test_df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6c354c69"
      },
      "source": [
        "**Reasoning**:\n",
        "I have successfully created new features. Now I need to evaluate these features by visualizing their relationship with the target variable and checking for potential multicollinearity. I will start by visualizing the relationship between some of the new features and the target variable.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "57bad7ad",
        "outputId": "94963a3a-e7e1-41c7-896c-c5ad3c953bb9"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Visualize the relationship between some new features and electricity consumption\n",
        "fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(15, 10))\n",
        "\n",
        "sns.scatterplot(ax=axes[0, 0], x=\"temp_daylight_interaction\", y=\"electricity_consumption\", data=train_df, alpha=0.5)\n",
        "axes[0, 0].set_title(\"Electricity Consumption vs. Temperature-Daylight Interaction\")\n",
        "\n",
        "sns.scatterplot(ax=axes[0, 1], x=\"temp_sunshine_interaction\", y=\"electricity_consumption\", data=train_df, alpha=0.5)\n",
        "axes[0, 1].set_title(\"Electricity Consumption vs. Temperature-Sunshine Interaction\")\n",
        "\n",
        "sns.scatterplot(ax=axes[1, 0], x=\"temperature_2m_max_sq\", y=\"electricity_consumption\", data=train_df, alpha=0.5)\n",
        "axes[1, 0].set_title(\"Electricity Consumption vs. Max Temperature Squared\")\n",
        "\n",
        "sns.scatterplot(ax=axes[1, 1], x=\" बिजली_consumption_lag_1\", y=\"electricity_consumption\", data=train_df, alpha=0.5)\n",
        "axes[1, 1].set_title(\"Electricity Consumption vs. Lag 1 Consumption\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2e7f10c2"
      },
      "source": [
        "**Reasoning**:\n",
        "I have visualized some of the new features. Now I will check for multicollinearity among the features to avoid issues in the model. I will calculate and visualize the correlation matrix.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "9pNqj7qh6g1H",
        "outputId": "ccc2426a-c768-4bb5-ec92-bb4ca7a663ef"
      },
      "outputs": [],
      "source": [
        "# Check for multicollinearity\n",
        "correlation_matrix = train_df[features + [\"temp_daylight_interaction\", \"temp_sunshine_interaction\", \"temperature_2m_max_sq\", \"temperature_2m_min_sq\", \" बिजली_consumption_lag_1\"]].corr()\n",
        "\n",
        "plt.figure(figsize=(12, 10))\n",
        "sns.heatmap(correlation_matrix, cmap=\"coolwarm\", annot=False)\n",
        "plt.title(\"Correlation Matrix of Features\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8e7ac0d"
      },
      "source": [
        "**Reasoning**:\n",
        "Define the parameter space for RandomizedSearchCV and perform the search to find the best hyperparameters for the LightGBM model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0c793965"
      },
      "source": [
        "## Model hyperparameter tuning\n",
        "\n",
        "### Subtask:\n",
        "Optimize the hyperparameters of the LightGBM model using techniques like GridSearchCV or RandomizedSearchCV to find the best configuration for lower RMSE and better generalization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65b23d42"
      },
      "source": [
        "**Reasoning**:\n",
        "Define the parameter space for RandomizedSearchCV and perform the search to find the best hyperparameters for the LightGBM model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "817a5ad2",
        "outputId": "c47b431b-bf38-428c-d798-edc49dc3ea88"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "import scipy.stats as st\n",
        "\n",
        "# Define the parameter space for RandomizedSearchCV\n",
        "param_distributions = {\n",
        "    'n_estimators': st.randint(100, 1500),  # Rentang lebih luas\n",
        "    'learning_rate': st.loguniform(0.0001, 0.1), # Menggunakan loguniform\n",
        "    'num_leaves': st.randint(10, 100),      # Rentang lebih luas\n",
        "    'max_depth': [-1] + list(st.randint(3, 20).rvs(size=10)), # Rentang lebih luas & lebih banyak sampel\n",
        "    'min_child_samples': st.randint(1, 100), # Rentang sangat luas\n",
        "    'subsample': st.uniform(0.4, 0.6),      # Rentang dari 0.4 hingga 1.0\n",
        "    'colsample_bytree': st.uniform(0.4, 0.6), # Rentang dari 0.4 hingga 1.0\n",
        "    'lambda_l1': st.loguniform(0.00001, 0.1), # Menggunakan loguniform\n",
        "    'lambda_l2': st.loguniform(0.00001, 0.1), # Menggunakan loguniform\n",
        "}\n",
        "\n",
        "# Re-define features to include the new engineered features\n",
        "features = sorted(list(set(common_features).union(set([\"temp_daylight_interaction\", \"temp_sunshine_interaction\", \"temperature_2m_max_sq\", \"temperature_2m_min_sq\", \"electricity_consumption_lag_1\"]))))\n",
        "\n",
        "# Ensure all features are in the training data\n",
        "common_features_with_engineered = list(set(features) & set(train_df.columns))\n",
        "\n",
        "X = train_df[common_features_with_engineered]\n",
        "y = train_df[\"electricity_consumption\"]\n",
        "\n",
        "# Initialize LightGBM regressor\n",
        "lgbm = lgb.LGBMRegressor(random_state=42)\n",
        "\n",
        "# Initialize RandomizedSearchCV\n",
        "# Using fewer iterations for demonstration purposes, ideally increase n_iter\n",
        "random_search = RandomizedSearchCV(\n",
        "    estimator=lgbm,\n",
        "    param_distributions=param_distributions,\n",
        "    n_iter=1000, # Number of parameter settings that are sampled\n",
        "    scoring='neg_root_mean_squared_error',\n",
        "    cv=KFold(n_splits=5, shuffle=True, random_state=42),\n",
        "    verbose=1,\n",
        "    n_jobs=-1,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Fit RandomizedSearchCV\n",
        "random_search.fit(X, y)\n",
        "\n",
        "# Print the best hyperparameters\n",
        "print(\"Best hyperparameters found by RandomizedSearchCV:\")\n",
        "print(random_search.best_params_)\n",
        "\n",
        "# Retrieve the best model\n",
        "best_lgbm_model = random_search.best_estimator_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1d9012c2"
      },
      "source": [
        "## Cross-Validation Strategy Review\n",
        "\n",
        "### Subtask:\n",
        "Evaluate the current KFold cross-validation setup and use the best model found during hyperparameter tuning to get a more reliable estimate of its performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4de6bded"
      },
      "source": [
        "**Reasoning**:\n",
        "Use the best model from RandomizedSearchCV and evaluate its performance using KFold cross-validation to get a robust estimate of the RMSE."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "86642346",
        "outputId": "3687fc82-0fd3-4f3a-9878-c79a47e3e844"
      },
      "outputs": [],
      "source": [
        "# Use the best model found by RandomizedSearchCV\n",
        "model = best_lgbm_model\n",
        "\n",
        "# Re-define features to include the new engineered features\n",
        "features = sorted(list(set(common_features).union(set([\"temp_daylight_interaction\", \"temp_sunshine_interaction\", \"temperature_2m_max_sq\", \"temperature_2m_min_sq\", \"electricity_consumption_lag_1\"]))))\n",
        "\n",
        "# Ensure all features are in the training data\n",
        "common_features_with_engineered = list(set(features) & set(train_df.columns))\n",
        "\n",
        "X = train_df[common_features_with_engineered]\n",
        "y = train_df[\"electricity_consumption\"]\n",
        "\n",
        "# Ensure the test data has the same features as the training data\n",
        "X_test = test_df[common_features_with_engineered]\n",
        "\n",
        "\n",
        "# LightGBM Model Training with Cross-Validation using the best model\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "oof_preds = np.zeros(len(X))\n",
        "test_preds = np.zeros(len(X_test))\n",
        "\n",
        "for fold, (train_index, val_index) in enumerate(kf.split(X, y)):\n",
        "    print(f\"Fold {fold+1}\")\n",
        "    X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n",
        "    y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
        "\n",
        "    # Train the model with the best hyperparameters\n",
        "    model.fit(X_train, y_train,\n",
        "              eval_set=[(X_val, y_val)],\n",
        "              eval_metric='rmse',\n",
        "              callbacks=[lgb.early_stopping(20, verbose=False)])\n",
        "\n",
        "    oof_preds[val_index] = model.predict(X_val)\n",
        "    test_preds += model.predict(X_test) / kf.n_splits\n",
        "\n",
        "# Evaluate OOF predictions\n",
        "rmse_oof = np.sqrt(mean_squared_error(y, oof_preds))\n",
        "print(f\"Overall OOF RMSE with optimized hyperparameters: {rmse_oof}\")\n",
        "\n",
        "# Prepare submission file with predictions from the optimized model\n",
        "submission_df = pd.DataFrame({\n",
        "    \"ID\": test_df[\"ID\"],\n",
        "    \"electricity_consumption\": test_preds\n",
        "})\n",
        "\n",
        "# Ensure predictions are non-negative\n",
        "submission_df[\"electricity_consumption\"] = submission_df[\"electricity_consumption\"].apply(lambda x: max(0, x))\n",
        "\n",
        "submission_df.to_csv(\"submission_optimized.csv\", index=False)\n",
        "\n",
        "print(\"Model evaluation with optimized hyperparameters and submission file generation complete.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9411909"
      },
      "source": [
        "# Task\n",
        "Optimize the RMSE of the LightGBM model to be between 10 and 20 and prevent overfitting, ensuring accurate predictions for specific examples like 'cluster_1_2022-01-01,0', 'cluster_2_2022-01-01,0', and 'cluster_3_2022-01-01,0', by continuing the hyperparameter tuning with n_iter=10, implementing regularization techniques, exploring ensemble methods, analyzing feature importance, performing prediction analysis, and iteratively refining the model before final training and prediction."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d361ef47"
      },
      "source": [
        "## Regularization techniques\n",
        "\n",
        "### Subtask:\n",
        "Implement additional regularization techniques within the LightGBM model or explore other models that inherently handle overfitting better.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "774d0dbf"
      },
      "source": [
        "**Reasoning**:\n",
        "I will implement additional regularization techniques by adding `lambda_l1`, `lambda_l2`, and `min_child_samples` to the `lgb_params` and retrain the model with KFold cross-validation to evaluate the impact on RMSE.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbfe2095"
      },
      "source": [
        "**Reasoning**:\n",
        "I have added regularization and re-trained the model. The next step is to experiment with different values for the regularization parameters to see their impact on the RMSE. I will try a different set of values for lambda_l1, lambda_l2, and min_child_samples and re-run the training and evaluation.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8d7c1c16"
      },
      "source": [
        "**Reasoning**:\n",
        "I have experimented with two sets of regularization parameters, and the RMSE has not significantly improved. I will now try a third set of values for `lambda_l1`, `lambda_l2`, and `min_child_samples` to further explore the impact of regularization.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8b2b00aa"
      },
      "source": [
        "**Reasoning**:\n",
        "I will implement additional regularization techniques by adding `lambda_l1`, `lambda_l2`, and `min_child_samples` to the `lgb_params` and retrain the model with KFold cross-validation to evaluate the impact on RMSE."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dbee91f5"
      },
      "source": [
        "## Ensemble Methods\n",
        "\n",
        "### Subtask:\n",
        "Experiment with combining multiple models (e.g., stacking, bagging, boosting) to leverage their individual strengths and potentially improve overall performance and reduce variance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6b4620f5"
      },
      "source": [
        "**Reasoning**:\n",
        "I will start by implementing a simple ensemble of the LightGBM model trained with different random states in the cross-validation process. This can help reduce variance and potentially improve the overall RMSE."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ee61ef8",
        "outputId": "02f29fcf-12d4-43ea-bb5c-4d743b16deca"
      },
      "outputs": [],
      "source": [
        "# Re-define features to include the new engineered features\n",
        "features = sorted(list(set(common_features).union(set([\"temp_daylight_interaction\", \"temp_sunshine_interaction\", \"temperature_2m_max_sq\", \"temperature_2m_min_sq\", \"electricity_consumption_lag_1\"]))))\n",
        "\n",
        "# Ensure all features are in the training data\n",
        "common_features_with_engineered = list(set(features) & set(train_df.columns))\n",
        "\n",
        "X = train_df[common_features_with_engineered]\n",
        "y = train_df[\"electricity_consumption\"]\n",
        "\n",
        "# Ensure the test data has the same features as the training data\n",
        "X_test = test_df[common_features_with_engineered]\n",
        "\n",
        "# LightGBM Model Training with Cross-Validation using the best model and regularization\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "oof_preds = np.zeros(len(X))\n",
        "test_preds = np.zeros(len(X_test))\n",
        "models = []\n",
        "\n",
        "for fold, (train_index, val_index) in enumerate(kf.split(X, y)):\n",
        "    print(f\"Fold {fold+1}\")\n",
        "    X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n",
        "    y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
        "\n",
        "    # Using the best hyperparameters found earlier, with added regularization\n",
        "    lgb_params = {\n",
        "        'objective': 'regression_l1',\n",
        "        'metric': 'rmse',\n",
        "        'n_estimators': 1000,\n",
        "        'learning_rate': 0.05,\n",
        "        'feature_fraction': 0.8,\n",
        "        'bagging_fraction': 0.8,\n",
        "        'bagging_freq': 1,\n",
        "        'lambda_l1': 0.1,\n",
        "        'lambda_l2': 0.1,\n",
        "        'min_child_samples': 20,\n",
        "        'num_leaves': 31,\n",
        "        'verbose': -1,\n",
        "        'n_jobs': -1,\n",
        "        'seed': 42 + fold  # Vary the seed for each fold\n",
        "    }\n",
        "\n",
        "\n",
        "    model = lgb.LGBMRegressor(**lgb_params)\n",
        "    model.fit(X_train, y_train,\n",
        "              eval_set=[(X_val, y_val)],\n",
        "              eval_metric='rmse',\n",
        "              callbacks=[lgb.early_stopping(20, verbose=False)])\n",
        "\n",
        "    oof_preds[val_index] = model.predict(X_val)\n",
        "    test_preds += model.predict(X_test) / kf.n_splits\n",
        "    models.append(model)\n",
        "\n",
        "\n",
        "# Evaluate OOF predictions\n",
        "rmse_oof = np.sqrt(mean_squared_error(y, oof_preds))\n",
        "print(f\"Overall OOF RMSE with ensembling (averaged folds): {rmse_oof}\")\n",
        "\n",
        "# Prepare submission file with predictions from the ensembled model\n",
        "submission_df = pd.DataFrame({\n",
        "    \"ID\": test_df[\"ID\"],\n",
        "    \"electricity_consumption\": test_preds\n",
        "})\n",
        "\n",
        "# Ensure predictions are non-negative\n",
        "submission_df[\"electricity_consumption\"] = submission_df[\"electricity_consumption\"].apply(lambda x: max(0, x))\n",
        "\n",
        "submission_df.to_csv(\"submission_ensembled.csv\", index=False)\n",
        "\n",
        "print(\"Ensemble model training and submission file generation complete.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d33b8c87"
      },
      "source": [
        "## Analyze Feature Importance\n",
        "\n",
        "### Subtask:\n",
        "Examine the importance of different features in the trained model to identify potentially irrelevant or noisy features that could be removed or transformed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "64b52927"
      },
      "source": [
        "**Reasoning**:\n",
        "Analyze the feature importance from the trained LightGBM model to understand which features contribute most to the predictions and identify potentially less important features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "b9f25f08",
        "outputId": "e404a99c-976c-46bf-8150-f155d28570dc"
      },
      "source": [
        "from matplotlib.pyplot import plt\n",
        "\n",
        "# Get feature importances from the trained model\n",
        "# Since we used cross-validation, we can average the feature importances across the folds\n",
        "feature_importances = pd.DataFrame()\n",
        "for i, model in enumerate(models):\n",
        "    fold_importance = pd.DataFrame({\n",
        "        'feature': model.feature_name_,\n",
        "        'importance': model.feature_importances_\n",
        "    })\n",
        "    fold_importance['fold'] = i + 1\n",
        "    feature_importances = pd.concat([feature_importances, fold_importance], axis=0)\n",
        "\n",
        "# Calculate the average feature importance across all folds\n",
        "average_feature_importance = feature_importances.groupby('feature')['importance'].mean().sort_values(ascending=False)\n",
        "\n",
        "# Display the top 20 most important features\n",
        "print(\"Top 20 Most Important Features:\")\n",
        "print(average_feature_importance.head(20))\n",
        "\n",
        "# Visualize feature importances\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.barplot(x=average_feature_importance.head(20).values, y=average_feature_importance.head(20).index)\n",
        "plt.title('Top 20 Feature Importances (Averaged across folds)')\n",
        "plt.xlabel('Importance')\n",
        "plt.ylabel('Feature')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "351f5340"
      },
      "source": [
        "## Prediction Analysis\n",
        "\n",
        "### Subtask:\n",
        "Analyze the predictions for specific examples like 'cluster_1_2022-01-01', 'cluster_2_2022-01-01', and 'cluster_3_2022-01-01' to understand where the model is making errors and inform further improvements."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "149aa393"
      },
      "source": [
        "**Reasoning**:\n",
        "Filter the test set to include the specified examples and display their actual values and the model's predictions to understand the prediction accuracy for these specific cases."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 161
        },
        "id": "acef00fc",
        "outputId": "78e16498-d20e-416e-8458-16908cab6a87"
      },
      "outputs": [],
      "source": [
        "# Filter test data for the specific examples\n",
        "specific_examples_ids = [\n",
        "    \"cluster_1_2022-01-01\",\n",
        "    \"cluster_2_2022-01-01\",\n",
        "    \"cluster_3_2022-01-01\",\n",
        "]\n",
        "\n",
        "# Assuming test_df has the original structure and submission_df has the predictions\n",
        "# Merge test_df with submission_df to get predictions alongside original data\n",
        "test_with_preds = test_df.merge(submission_df, on=\"ID\", how=\"left\")\n",
        "\n",
        "# Filter for the specific examples\n",
        "specific_predictions = test_with_preds[test_with_preds[\"ID\"].isin(specific_examples_ids)]\n",
        "\n",
        "# Display the original data and predictions for these examples\n",
        "print(\"Predictions for specific examples:\")\n",
        "# Display only the relevant columns: ID, date, and the predicted electricity_consumption\n",
        "display(specific_predictions[[\"ID\", \"date\", \"electricity_consumption\"]])\n",
        "\n",
        "# Note: The test set does not have the actual 'electricity_consumption', so we can only show the predicted value.\n",
        "# To truly analyze accuracy for these specific examples, you would need their actual values from a held-out set or the original source."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d974668a"
      },
      "source": [
        "# Task\n",
        "Refine the current model iteratively to achieve an RMSE between 10 and 20 without overfitting, ensuring accurate predictions. This involves further feature engineering, exploring different model architectures, implementing advanced ensembling techniques, and further hyperparameter tuning. After refinement, train the final model on the entire training dataset and generate predictions for the test set. Finally, present the findings in a readable format."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "69f6dcd7"
      },
      "source": [
        "## Iterative refinement\n",
        "\n",
        "### Subtask:\n",
        "Based on the insights from feature importance and prediction analysis, iteratively refine the model. This could involve further feature engineering (e.g., creating more complex interactions, seasonal features, or lag features), exploring different model architectures (e.g., other boosting models like XGBoost or CatBoost, or time series specific models), implementing more advanced ensembling techniques (e.g., stacking or weighted averaging based on model performance), and further hyperparameter tuning with a larger search space or more iterations.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "97ec5e51"
      },
      "source": [
        "**Reasoning**:\n",
        "Based on the feature importance, 'day_of_year', 'wind_direction_10m_dominant', and 'daylight_duration' are among the most important features. I will create additional interaction features and seasonal features to capture more complex relationships and also add more lag features. Then I will retrain the LightGBM model with the best hyperparameters found earlier and evaluate its performance with cross-validation.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a26937ca"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous command failed because the lag features for electricity consumption were only created for the training data and thus are not present in the test data when trying to align the features. I need to ensure that the feature list used for selecting columns from the test DataFrame only includes features that exist in both DataFrames. I will also fix the future warning regarding `fillna` by using `bfill()` and `ffill()` directly.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r9BpXQgS-U5T",
        "outputId": "7528f1a0-435c-4830-d219-6be177eb8611"
      },
      "outputs": [],
      "source": [
        "# Create more interaction features\n",
        "train_df[\"temp_wind_interaction\"] = train_df[\"temperature_2m_max\"] * train_df[\"wind_speed_10m_max\"]\n",
        "test_df[\"temp_wind_interaction\"] = test_df[\"temperature_2m_max\"] * test_df[\"wind_speed_10m_max\"]\n",
        "\n",
        "# Create seasonal features using sine and cosine transformations of day of year\n",
        "train_df[\"day_of_year_sin\"] = np.sin(2 * np.pi * train_df[\"day_of_year\"] / 365)\n",
        "train_df[\"day_of_year_cos\"] = np.cos(2 * np.pi * train_df[\"day_of_year\"] / 365)\n",
        "test_df[\"day_of_year_sin\"] = np.sin(2 * np.pi * test_df[\"day_of_year\"] / 365)\n",
        "test_df[\"day_of_year_cos\"] = np.cos(2 * np.pi * test_df[\"day_of_year\"] / 365)\n",
        "\n",
        "# Create additional lag features for electricity consumption (training data only)\n",
        "train_df[\"electricity_consumption_lag_7\"] = train_df.groupby(cluster_cols)[\"electricity_consumption\"].shift(7)\n",
        "train_df[\"electricity_consumption_lag_30\"] = train_df.groupby(cluster_cols)[\"electricity_consumption\"].shift(30)\n",
        "\n",
        "# Create additional lag features for temperature (test data only)\n",
        "test_df[\"temperature_2m_max_lag_7\"] = test_df.groupby(cluster_cols)[\"temperature_2m_max\"].shift(7)\n",
        "test_df[\"temperature_2m_max_lag_30\"] = test_df.groupby(cluster_cols)[\"temperature_2m_max\"].shift(30)\n",
        "\n",
        "\n",
        "# Fill NaNs created by shifting\n",
        "train_df.bfill(inplace=True)\n",
        "test_df.bfill(inplace=True)\n",
        "\n",
        "train_df.ffill(inplace=True)\n",
        "test_df.ffill(inplace=True)\n",
        "\n",
        "# Re-define features to include the new engineered features\n",
        "engineered_features = [col for col in train_df.columns if col not in [\"ID\", \"date\", \"electricity_consumption\"] and not col.startswith(\"cluster_\")]\n",
        "cluster_cols = [col for col in train_df.columns if col.startswith(\"cluster_\")]\n",
        "features = sorted(list(set(engineered_features).union(set(cluster_cols))))\n",
        "\n",
        "# Ensure all features are in both training and test dataframes\n",
        "common_features_with_engineered = list(set(features) & set(train_df.columns) & set(test_df.columns))\n",
        "\n",
        "X = train_df[common_features_with_engineered]\n",
        "y = train_df[\"electricity_consumption\"]\n",
        "\n",
        "# Ensure the test data has the same common features\n",
        "X_test = test_df[common_features_with_engineered]\n",
        "\n",
        "# LightGBM Model Training with Cross-Validation using the best model and regularization\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "oof_preds = np.zeros(len(X))\n",
        "test_preds = np.zeros(len(X_test))\n",
        "models = []\n",
        "\n",
        "# Using the best hyperparameters found earlier\n",
        "best_lgb_params = random_search.best_params_\n",
        "best_lgb_params['verbose'] = -1 # Set verbose to -1 to avoid printing progress for each fold\n",
        "best_lgb_params['seed'] = 42 # Set a base seed\n",
        "\n",
        "for fold, (train_index, val_index) in enumerate(kf.split(X, y)):\n",
        "    print(f\"Fold {fold+1}\")\n",
        "    X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n",
        "    y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
        "\n",
        "    model = lgb.LGBMRegressor(**best_lgb_params)\n",
        "    model.fit(X_train, y_train,\n",
        "              eval_set=[(X_val, y_val)],\n",
        "              eval_metric='rmse',\n",
        "              callbacks=[lgb.early_stopping(20, verbose=False)])\n",
        "\n",
        "    oof_preds[val_index] = model.predict(X_val)\n",
        "    test_preds += model.predict(X_test) / kf.n_splits\n",
        "    models.append(model)\n",
        "\n",
        "\n",
        "# Evaluate OOF predictions\n",
        "rmse_oof = np.sqrt(mean_squared_error(y, oof_preds))\n",
        "print(f\"Overall OOF RMSE with new features and optimized hyperparameters: {rmse_oof}\")\n",
        "\n",
        "# Prepare submission file with predictions\n",
        "submission_df = pd.DataFrame({\n",
        "    \"ID\": test_df[\"ID\"],\n",
        "    \"electricity_consumption\": test_preds\n",
        "})\n",
        "\n",
        "# Ensure predictions are non-negative\n",
        "submission_df[\"electricity_consumption\"] = submission_df[\"electricity_consumption\"].apply(lambda x: max(0, x))\n",
        "\n",
        "submission_df.to_csv(\"submission_refined_features.csv\", index=False)\n",
        "\n",
        "print(\"Model training with refined features and submission file generation complete.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "01201ce1"
      },
      "source": [
        "**Reasoning**:\n",
        "I have added new features and retrained the model, resulting in a slight improvement in RMSE. Now, I will analyze the feature importances again with the updated feature set to see how the new features are contributing. This will help inform further feature engineering or model tuning.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "c9e3587c",
        "outputId": "c91ec85d-46cb-47f3-b614-5ef1584598b7"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# Get feature importances from the trained model\n",
        "# Since we used cross-validation, we can average the feature importances across the folds\n",
        "feature_importances = pd.DataFrame()\n",
        "for i, model in enumerate(models):\n",
        "    fold_importance = pd.DataFrame({\n",
        "        'feature': model.feature_name_,\n",
        "        'importance': model.feature_importances_\n",
        "    })\n",
        "    fold_importance['fold'] = i + 1\n",
        "    feature_importances = pd.concat([feature_importances, fold_importance], axis=0)\n",
        "\n",
        "# Calculate the average feature importance across all folds\n",
        "average_feature_importance = feature_importances.groupby('feature')['importance'].mean().sort_values(ascending=False)\n",
        "\n",
        "# Display the top 20 most important features\n",
        "print(\"Top 20 Most Important Features:\")\n",
        "print(average_feature_importance.head(20))\n",
        "\n",
        "# Visualize feature importances\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.barplot(x=average_feature_importance.head(20).values, y=average_feature_importance.head(20).index)\n",
        "plt.title('Top 20 Feature Importances (Averaged across folds)')\n",
        "plt.xlabel('Importance')\n",
        "plt.ylabel('Feature')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e606cf11"
      },
      "source": [
        "**Reasoning**:\n",
        "Based on the updated feature importance, the seasonal features (`day_of_year_sin`, `day_of_year_cos`), `year`, and `daylight_duration` are highly important. The lag features for electricity consumption are not in the top 20, possibly due to how missing values were handled initially. The RMSE is still above the target range (10-20). I will experiment with adding more specific lag features and rolling window features, particularly for temperature and wind speed, as these also show good importance. I will also slightly adjust the hyperparameters for the LightGBM model and retrain with cross-validation.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0e1a7d1b"
      },
      "source": [
        "**Reasoning**:\n",
        "The rolling window calculation with `reset_index(level=0, drop=True)` seems to be causing an index alignment issue when assigning it back to the DataFrame. I need to ensure the index of the rolling window result aligns correctly with the original DataFrame index. I will try a different approach for creating rolling window features by applying the rolling function directly and then handling the index explicitly. I will also keep the slightly adjusted hyperparameters.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vrkOhWnp-gRg",
        "outputId": "e1d7eff2-dbb7-46a6-e709-77227ffb99fb"
      },
      "outputs": [],
      "source": [
        "# Create more specific lag features for temperature and wind speed\n",
        "train_df[\"temperature_2m_max_lag_1\"] = train_df.groupby(cluster_cols)[\"temperature_2m_max\"].shift(1)\n",
        "train_df[\"temperature_2m_max_lag_7\"] = train_df.groupby(cluster_cols)[\"temperature_2m_max\"].shift(7)\n",
        "train_df[\"temperature_2m_min_lag_1\"] = train_df.groupby(cluster_cols)[\"temperature_2m_min\"].shift(1)\n",
        "train_df[\"temperature_2m_min_lag_7\"] = train_df.groupby(cluster_cols)[\"temperature_2m_min\"].shift(7)\n",
        "train_df[\"wind_speed_10m_max_lag_1\"] = train_df.groupby(cluster_cols)[\"wind_speed_10m_max\"].shift(1)\n",
        "train_df[\"wind_speed_10m_max_lag_7\"] = train_df.groupby(cluster_cols)[\"wind_speed_10m_max\"].shift(7)\n",
        "\n",
        "test_df[\"temperature_2m_max_lag_1\"] = test_df.groupby(cluster_cols)[\"temperature_2m_max\"].shift(1)\n",
        "test_df[\"temperature_2m_max_lag_7\"] = test_df.groupby(cluster_cols)[\"temperature_2m_max\"].shift(7)\n",
        "test_df[\"temperature_2m_min_lag_1\"] = test_df.groupby(cluster_cols)[\"temperature_2m_min\"].shift(1)\n",
        "test_df[\"temperature_2m_min_lag_7\"] = test_df.groupby(cluster_cols)[\"temperature_2m_min\"].shift(7)\n",
        "test_df[\"wind_speed_10m_max_lag_1\"] = test_df.groupby(cluster_cols)[\"wind_speed_10m_max\"].shift(1)\n",
        "test_df[\"wind_speed_10m_max_lag_7\"] = test_df.groupby(cluster_cols)[\"wind_speed_10m_max\"].shift(7)\n",
        "\n",
        "\n",
        "# Create rolling window features for temperature (e.g., 7-day rolling mean)\n",
        "train_df[\"temperature_2m_max_rolling_mean_7\"] = train_df.groupby(cluster_cols)[\"temperature_2m_max\"].transform(lambda x: x.rolling(window=7).mean())\n",
        "test_df[\"temperature_2m_max_rolling_mean_7\"] = test_df.groupby(cluster_cols)[\"temperature_2m_max\"].transform(lambda x: x.rolling(window=7).mean())\n",
        "\n",
        "train_df[\"temperature_2m_min_rolling_mean_7\"] = train_df.groupby(cluster_cols)[\"temperature_2m_min\"].transform(lambda x: x.rolling(window=7).mean())\n",
        "test_df[\"temperature_2m_min_rolling_mean_7\"] = test_df.groupby(cluster_cols)[\"temperature_2m_min\"].transform(lambda x: x.rolling(window=7).mean())\n",
        "\n",
        "\n",
        "# Fill NaNs created by shifting and rolling (including initial NaNs)\n",
        "train_df.bfill(inplace=True)\n",
        "test_df.bfill(inplace=True)\n",
        "\n",
        "train_df.ffill(inplace=True)\n",
        "test_df.ffill(inplace=True)\n",
        "\n",
        "\n",
        "# Re-define features to include the new engineered features\n",
        "engineered_features = [col for col in train_df.columns if col not in [\"ID\", \"date\", \"electricity_consumption\"] and not col.startswith(\"cluster_\")]\n",
        "cluster_cols = [col for col in train_df.columns if col.startswith(\"cluster_\")]\n",
        "features = sorted(list(set(engineered_features).union(set(cluster_cols))))\n",
        "\n",
        "\n",
        "# Ensure all features are in both training and test dataframes\n",
        "common_features_with_engineered = list(set(features) & set(train_df.columns) & set(test_df.columns))\n",
        "\n",
        "X = train_df[common_features_with_engineered]\n",
        "y = train_df[\"electricity_consumption\"]\n",
        "\n",
        "# Ensure the test data has the same common features\n",
        "X_test = test_df[common_features_with_engineered]\n",
        "\n",
        "# LightGBM Model Training with Cross-Validation using slightly adjusted hyperparameters\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "oof_preds = np.zeros(len(X))\n",
        "test_preds = np.zeros(len(X_test))\n",
        "models = []\n",
        "\n",
        "# Using the best hyperparameters found earlier\n",
        "best_lgb_params = random_search.best_params_\n",
        "best_lgb_params['verbose'] = -1 # Set verbose to -1 to avoid printing progress for each fold\n",
        "best_lgb_params['seed'] = 42 # Set a base seed\n",
        "best_lgb_params['n_estimators'] = 1200 # Increased estimators\n",
        "best_lgb_params['learning_rate'] = 0.04 # Slightly reduced learning rate\n",
        "best_lgb_params['num_leaves'] = 40 # Increased num_leaves\n",
        "\n",
        "\n",
        "for fold, (train_index, val_index) in enumerate(kf.split(X, y)):\n",
        "    print(f\"Fold {fold+1}\")\n",
        "    X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n",
        "    y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
        "\n",
        "    model = lgb.LGBMRegressor(**best_lgb_params)\n",
        "    model.fit(X_train, y_train,\n",
        "              eval_set=[(X_val, y_val)],\n",
        "              eval_metric='rmse',\n",
        "              callbacks=[lgb.early_stopping(30, verbose=False)]) # Increased early stopping rounds\n",
        "\n",
        "    oof_preds[val_index] = model.predict(X_val)\n",
        "    test_preds += model.predict(X_test) / kf.n_splits\n",
        "    models.append(model)\n",
        "\n",
        "\n",
        "# Evaluate OOF predictions\n",
        "rmse_oof = np.sqrt(mean_squared_error(y, oof_preds))\n",
        "print(f\"Overall OOF RMSE with more features and adjusted hyperparameters: {rmse_oof}\")\n",
        "\n",
        "# Prepare submission file with predictions\n",
        "submission_df = pd.DataFrame({\n",
        "    \"ID\": test_df[\"ID\"],\n",
        "    \"electricity_consumption\": test_preds\n",
        "})\n",
        "\n",
        "# Ensure predictions are non-negative\n",
        "submission_df[\"electricity_consumption\"] = submission_df[\"electricity_consumption\"].apply(lambda x: max(0, x))\n",
        "\n",
        "submission_df.to_csv(\"submission_more_features_v2.csv\", index=False)\n",
        "\n",
        "print(\"Model training with more refined features (v2) and submission file generation complete.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "63163ac9"
      },
      "source": [
        "**Reasoning**:\n",
        "The RMSE is now within the target range (10-20). I will perform prediction analysis on the specific examples again with the refined model to check if the predictions for these cases have improved.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 161
        },
        "id": "14d3396b",
        "outputId": "9b72a51a-7c18-4a51-fbb0-94a921718db8"
      },
      "outputs": [],
      "source": [
        "# Filter test data for the specific examples\n",
        "specific_examples_ids = [\n",
        "    \"cluster_1_2022-01-01\",\n",
        "    \"cluster_2_2022-01-01\",\n",
        "    \"cluster_3_2022-01-01\",\n",
        "]\n",
        "\n",
        "# Merge test_df with the latest submission_df to get predictions alongside original data\n",
        "test_with_preds_v2 = test_df.merge(submission_df, on=\"ID\", how=\"left\")\n",
        "\n",
        "# Filter for the specific examples\n",
        "specific_predictions_v2 = test_with_preds_v2[test_with_preds_v2[\"ID\"].isin(specific_examples_ids)]\n",
        "\n",
        "# Display the original data and predictions for these examples\n",
        "print(\"Predictions for specific examples with refined model:\")\n",
        "# Display only the relevant columns: ID, date, and the predicted electricity_consumption\n",
        "display(specific_predictions_v2[[\"ID\", \"date\", \"electricity_consumption\"]])\n",
        "\n",
        "# Note: The test set still does not have the actual 'electricity_consumption'."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d54cc320"
      },
      "source": [
        "## Final model training and prediction\n",
        "\n",
        "### Subtask:\n",
        "Train the final LightGBM model with the best hyperparameters and the refined feature set on the entire training dataset and generate predictions for the test dataset.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8cf28b00"
      },
      "source": [
        "**Reasoning**:\n",
        "Train the final LightGBM model with the best hyperparameters and the refined feature set on the entire training dataset and generate predictions for the test dataset, then save the submission file.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "66afb06c",
        "outputId": "e52f3697-34bb-4d65-ed86-bb99e4f64897"
      },
      "outputs": [],
      "source": [
        "# Re-define features to include all relevant engineered features and cluster columns\n",
        "engineered_features = [col for col in train_df.columns if col not in [\"ID\", \"date\", \"electricity_consumption\"] and not col.startswith(\"cluster_\")]\n",
        "cluster_cols = [col for col in train_df.columns if col.startswith(\"cluster_\")]\n",
        "features = sorted(list(set(engineered_features).union(set(cluster_cols))))\n",
        "\n",
        "# Ensure all features are in both training and test dataframes\n",
        "common_features_with_engineered = list(set(features) & set(train_df.columns) & set(test_df.columns))\n",
        "\n",
        "# Create the feature matrix X and target vector y using the entire train_df\n",
        "X = train_df[common_features_with_engineered]\n",
        "y = train_df[\"electricity_consumption\"]\n",
        "\n",
        "# Create the feature matrix X_test using the test_df with the same final feature set\n",
        "X_test = test_df[common_features_with_engineered]\n",
        "\n",
        "# Initialize the LightGBM regressor model using the best hyperparameters\n",
        "# Using the best hyperparameters found during the iterative refinement process\n",
        "best_lgb_params = random_search.best_params_\n",
        "best_lgb_params['verbose'] = -1 # Set verbose to -1 to avoid printing progress\n",
        "\n",
        "final_model = lgb.LGBMRegressor(**best_lgb_params)\n",
        "\n",
        "# Train the model on the entire training data\n",
        "final_model.fit(X, y)\n",
        "\n",
        "# Generate predictions on the test set\n",
        "test_preds = final_model.predict(X_test)\n",
        "\n",
        "# Create a submission DataFrame\n",
        "submission_df = pd.DataFrame({\n",
        "    \"ID\": test_df[\"ID\"],\n",
        "    \"electricity_consumption\": test_preds\n",
        "})\n",
        "\n",
        "# Ensure predictions are non-negative\n",
        "submission_df[\"electricity_consumption\"] = submission_df[\"electricity_consumption\"].apply(lambda x: max(0, x))\n",
        "\n",
        "# Save the submission DataFrame to a CSV file\n",
        "submission_df.to_csv(\"submission_final.csv\", index=False)\n",
        "\n",
        "print(\"Final model training and submission file generation complete.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0b340d32"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   The iterative refinement process successfully achieved an Out-of-Fold RMSE of approximately 17.30 on the training data with cross-validation, which falls within the target range of 10 to 20.\n",
        "*   Feature engineering, including adding interaction features (`temp_wind_interaction`), seasonal features (`day_of_year_sin`, `day_of_year_cos`), specific lag features (lag 1 and 7 for temperature and wind speed), and rolling window mean features (7-day for temperature), was crucial in improving the model's performance.\n",
        "*   Seasonal features (`day_of_year_sin`, `day_of_year_cos`), `year`, and `daylight_duration` consistently appeared as highly important features in the model.\n",
        "*   Training the final LightGBM model on the entire training dataset with the refined feature set and optimized hyperparameters successfully generated predictions for the test set, saved to \"submission\\_final.csv\".\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   The achieved OOF RMSE of 17.30 suggests that the model has a reasonable balance between bias and variance on the training data. Further validation on an independent test set (e.g., a public leaderboard) is necessary to confirm its generalization performance and ensure the target RMSE is met on unseen data.\n",
        "*   Analyzing the predictions on the specific test examples (e.g., 'cluster\\_1\\_2022-01-01') can provide insights into how the model behaves for different clusters and dates, which could guide further targeted feature engineering or model adjustments if needed.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "380c9896"
      },
      "source": [
        "Untuk mendapatkan file `submission_final.csv`, saya melakukan langkah-langkah berikut:\n",
        "\n",
        "1.  **Pemuatan Data dan Eksplorasi Awal**: Saya memuat dataset pelatihan dan pengujian (`train.csv` dan `test.csv`) ke dalam pandas DataFrame dan melakukan eksplorasi awal untuk memahami struktur, tipe data, dan statistik dasar dari data tersebut.\n",
        "2.  **Rekayasa Fitur (Feature Engineering)**: Saya membuat fitur-fitur baru dari data yang ada untuk membantu model menangkap pola dan hubungan yang lebih kompleks. Ini termasuk:\n",
        "    *   Mengekstrak fitur berbasis waktu dari kolom 'date' (tahun, bulan, hari, hari dalam seminggu, hari dalam setahun, minggu dalam setahun, kuartal, dan apakah itu akhir pekan).\n",
        "    *   Membuat fitur 'temperature_range' sebagai selisih antara suhu maksimum dan minimum.\n",
        "    *   Melakukan one-hot encoding pada 'cluster_id' untuk merepresentasikan klaster kategorikal secara numerik.\n",
        "    *   Membuat fitur interaksi antara suhu dan durasi siang/sinar matahari (`temp_daylight_interaction`, `temp_sunshine_interaction`).\n",
        "    *   Membuat fitur polinomial untuk suhu (`temperature_2m_max_sq`, `temperature_2m_min_sq`).\n",
        "    *   Menghasilkan fitur lag untuk konsumsi listrik dan variabel suhu, dikelompokkan berdasarkan klaster, untuk menangkap ketergantungan temporal.\n",
        "    *   Membuat fitur musiman menggunakan transformasi sinus dan kosinus dari 'day_of_year'.\n",
        "    *   Membuat fitur interaksi tambahan, seperti antara suhu dan kecepatan angin (`temp_wind_interaction`).\n",
        "    *   Menghasilkan fitur rata-rata jendela bergulir (rolling window mean) untuk suhu.\n",
        "    *   Menangani nilai-nilai yang hilang yang dibuat oleh fitur lag dan rolling window menggunakan forward dan backward fill.\n",
        "3.  **Pemilihan Model**: Saya memilih LightGBM Regressor (`lgb.LGBMRegressor`) sebagai model utama karena efisiensi dan efektivitasnya dalam menangani data terstruktur. Saya juga awalnya mengeksplorasi RandomForestRegressor untuk perbandingan, tetapi LightGBM menunjukkan kinerja yang lebih baik.\n",
        "4.  **Penyetelan Hyperparameter (Hyperparameter Tuning)**: Untuk menemukan konfigurasi terbaik bagi model LightGBM, saya menggunakan `RandomizedSearchCV` untuk mencari ruang parameter yang ditentukan. Ini membantu mengoptimalkan parameter seperti `n_estimators`, `learning_rate`, `num_leaves`, `max_depth`, dan istilah regularisasi (`lambda_l1`, `lambda_l2`), dengan tujuan mencapai RMSE yang lebih rendah.\n",
        "5.  **Validasi Silang (Cross-Validation)**: Saya menggunakan KFold cross-validation dengan 5 lipatan (split) untuk mengevaluasi kinerja model secara kuat dan mendapatkan RMSE out-of-fold (OOF). Ini membantu dalam menilai seberapa baik model menggeneralisasi ke data yang belum terlihat dan memberikan perkiraan kinerja yang lebih andal daripada satu pembagian pelatihan-validasi.\n",
        "6.  **Penyempurnaan Iteratif (Iterative Refinement)**: Berdasarkan kinerja model awal (RMSE) dan wawasan dari analisis pentingnya fitur, saya secara iteratif menyempurnakan rekayasa fitur dan sedikit menyesuaikan hyperparameter model LightGBM. Ini termasuk menambahkan fitur lag yang lebih spesifik dan fitur rolling window, terutama untuk suhu dan kecepatan angin, yang diidentifikasi sebagai fitur penting.\n",
        "7.  **Pelatihan Model Akhir**: Setelah proses penyempurnaan iteratif menghasilkan RMSE OOF dalam rentang target (10-20), saya melatih model LightGBM akhir dengan hyperparameter terbaik dan set fitur lengkap yang direkayasa pada seluruh dataset pelatihan.\n",
        "8.  **Pembuatan Prediksi**: Menggunakan model akhir yang telah dilatih, saya menghasilkan prediksi untuk konsumsi listrik pada dataset pengujian.\n",
        "9.  **Pembuatan File Submission**: Akhirnya, saya membuat file `submission_final.csv` yang berisi 'ID' dari dataset pengujian dan prediksi 'electricity_consumption' yang dihasilkan, memastikan prediksi tidak bernilai negatif.\n",
        "\n",
        "Melalui langkah-langkah ini, saya bertujuan untuk membangun model yang kuat yang dapat menggeneralisasi dengan baik dan memberikan prediksi yang akurat untuk konsumsi listrik."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8e1044e9"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   The iterative refinement process successfully achieved an Out-of-Fold RMSE of approximately 17.30 on the training data with cross-validation, which falls within the target range of 10 to 20.\n",
        "*   Feature engineering, including adding interaction features (`temp_wind_interaction`), seasonal features (`day_of_year_sin`, `day_of_year_cos`), specific lag features (lag 1 and 7 for temperature and wind speed), and rolling window mean features (7-day for temperature), was crucial in improving the model's performance.\n",
        "*   Seasonal features (`day_of_year_sin`, `day_of_year_cos`), `year`, and `daylight_duration` consistently appeared as highly important features in the model.\n",
        "*   Training the final LightGBM model on the entire training dataset with the refined feature set and optimized hyperparameters successfully generated predictions for the test set, saved to \"submission\\_final.csv\".\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   The achieved OOF RMSE of 17.30 suggests that the model has a reasonable balance between bias and variance on the training data. Further validation on an independent test set (e.g., a public leaderboard) is necessary to confirm its generalization performance and ensure the target RMSE is met on unseen data.\n",
        "*   Analyzing the predictions on the specific test examples (e.g., 'cluster\\_1\\_2022-01-01') can provide insights into how the model behaves for different clusters and dates, which could guide further targeted feature engineering or model adjustments if needed."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
